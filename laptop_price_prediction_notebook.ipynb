{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658f3ad-6063-4bd3-9680-b8c7ac720293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "pip install seaborn\n",
    "pip install scikit-learn\n",
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf16899-061a-4700-af91-b4ee642f341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07d09c-c736-42fd-bb3e-21ff3271145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading file\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create an upload widget\n",
    "upload_button = widgets.FileUpload(\n",
    "    accept='.csv,.xlsx',\n",
    "    multiple=True          \n",
    ")\n",
    "\n",
    "# Display the upload button\n",
    "display(upload_button)\n",
    "\n",
    "# Access the uploaded file(s)\n",
    "def handle_upload(change):\n",
    "    for filename, file in upload_button.value.items():\n",
    "        print(f\"File {filename} uploaded successfully!\")\n",
    "\n",
    "upload_button.observe(handle_upload, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b41e9-2da7-414d-8c17-e48a9402c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Visualize price distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['Price'], kde=True)\n",
    "    plt.title('Distribution of Laptop Prices')\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "filepath = r'C:\\Users\\YourUsername\\Documents\\laptop.csv'\n",
    "df = load_and_explore_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0846c3f-1def-47c9-87c9-2e0a93adad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "def preprocess_laptop_data(df):\n",
    "    # Create a copy of the dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove unnamed columns\n",
    "    df = df.drop([col for col in df.columns if 'Unnamed' in col], axis=1)\n",
    "\n",
    "    # Remove null values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Basic preprocessing\n",
    "    df['Ram'] = df['Ram'].str.extract('(\\d+)').astype(float)\n",
    "    df['Weight'] = df['Weight'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "    # Create CPU features\n",
    "    df['CPU_Speed'] = df['Cpu'].str.extract('(\\d+\\.?\\d*)GHz').astype(float)\n",
    "    df['CPU_Brand'] = df['Cpu'].apply(lambda x:\n",
    "        'Intel' if 'Intel' in str(x) else\n",
    "        'AMD' if 'AMD' in str(x) else 'Other'\n",
    "    )\n",
    "\n",
    "    # Create GPU features\n",
    "    df['GPU_Brand'] = df['Gpu'].apply(lambda x:\n",
    "        'NVIDIA' if any(brand in str(x).upper() for brand in ['NVIDIA', 'GTX', 'RTX']) else\n",
    "        'AMD' if any(brand in str(x).upper() for brand in ['AMD', 'RADEON']) else\n",
    "        'Intel' if 'INTEL' in str(x).upper() else\n",
    "        'Other'\n",
    "    )\n",
    "\n",
    "    # Other features\n",
    "    df['TouchScreen'] = df['ScreenResolution'].apply(lambda x: 1 if 'Touch' in str(x) else 0)\n",
    "    df['SSD'] = df['Memory'].apply(lambda x: 1 if 'SSD' in str(x) else 0)\n",
    "    df['HDD'] = df['Memory'].apply(lambda x: 1 if 'HDD' in str(x) else 0)\n",
    "\n",
    "    # Process Company\n",
    "    major_companies = ['Apple', 'HP', 'Acer', 'Asus', 'Dell', 'Lenovo', 'MSI', 'Microsoft', 'Toshiba']\n",
    "    df['Company'] = df['Company'].apply(lambda x: x if x in major_companies else 'Other')\n",
    "\n",
    "    # Process OS\n",
    "    df['OpSys'] = df['OpSys'].apply(lambda x:\n",
    "        'Windows' if 'Windows' in str(x) else\n",
    "        'Mac' if 'Mac' in str(x) else\n",
    "        'Linux' if 'Linux' in str(x) else\n",
    "        'Chrome' if 'Chrome' in str(x) else\n",
    "        'No OS' if 'No OS' in str(x) else\n",
    "        'Other'\n",
    "    )\n",
    "    df.loc[df['Company'] == 'Apple', 'OpSys'] = 'Mac'\n",
    "\n",
    "    # Handle missing values in numerical features\n",
    "    numerical_features = ['Ram', 'Weight', 'CPU_Speed']\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "try:\n",
    "    df_processed = preprocess_laptop_data(df)\n",
    "    print(\"Processed dataframe shape:\", df_processed.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error during preprocessing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c90a8-30ab-4343-a327-244d32b2c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8c560-10b0-4a7d-abeb-9f50fadf8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training Data\n",
    "def prepare_training_data(df):\n",
    "\n",
    "    # Define features and target variable\n",
    "    features = ['Company', 'Ram', 'Weight', 'CPU_Brand', 'CPU_Speed',\n",
    "                'GPU_Brand', 'TouchScreen', 'OpSys', 'SSD', 'HDD']\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['Price']\n",
    "\n",
    "    # Define categorical and numerical feature lists\n",
    "    categorical_features = ['Company', 'CPU_Brand', 'GPU_Brand', 'OpSys']\n",
    "    numerical_features = ['Ram', 'Weight', 'CPU_Speed', 'TouchScreen', 'SSD', 'HDD']\n",
    "\n",
    "    # Create preprocessors for numerical and categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Other')),\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessors\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "# Prepare the training data\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test, preprocessor = prepare_training_data(df_processed)\n",
    "\n",
    "    # Transform the data\n",
    "    X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "    X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "    print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "    print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data preparation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be381d1f-a05a-4da2-920b-202b06869da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Models\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from typing import Dict, Any, Tuple\n",
    "from time import time\n",
    "\n",
    "def train_model_with_metrics(\n",
    "    model: Any,\n",
    "    model_name: str,\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    try:\n",
    "        # Start timer\n",
    "        start_time = time()\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = time() - start_time\n",
    "\n",
    "        # Store results\n",
    "        results = {\n",
    "            'model': model,\n",
    "            'r2': r2,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'training_time': training_time,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "        print(f\"MAE: ${mae:.2f}\")\n",
    "        print(f\"RMSE: ${rmse:.2f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def train_linear_regression(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        model = LinearRegression()\n",
    "        return train_model_with_metrics(model, \"Linear Regression\",\n",
    "                                      X_train, X_test, y_train, y_test)\n",
    "\n",
    "def train_random_forest(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    return train_model_with_metrics(model, \"Random Forest\",\n",
    "                                  X_train, X_test, y_train, y_test)\n",
    "\n",
    "def train_gradient_boosting(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        subsample=1.0,\n",
    "        max_features=None,\n",
    "        random_state=42\n",
    "    )\n",
    "    return train_model_with_metrics(model, \"Gradient Boosting\",\n",
    "                                  X_train, X_test, y_train, y_test)\n",
    "\n",
    "def train_all_models(\n",
    "    X_train_scaled: np.ndarray,\n",
    "    X_test_scaled: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Verify input shapes\n",
    "        print(\"\\nInput Data Shapes:\")\n",
    "        print(f\"X_train: {X_train_scaled.shape}\")\n",
    "        print(f\"X_test: {X_test_scaled.shape}\")\n",
    "        print(f\"y_train: {y_train.shape}\")\n",
    "        print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "        model_results = {}\n",
    "\n",
    "        # Train each model\n",
    "        models = {\n",
    "            'Linear Regression': train_linear_regression,\n",
    "            'Random Forest': train_random_forest,\n",
    "            'Gradient Boosting': train_gradient_boosting\n",
    "        }\n",
    "\n",
    "        for name, train_func in models.items():\n",
    "            results = train_func(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "            if results is not None:\n",
    "                model_results[name] = results\n",
    "\n",
    "        # Find best model\n",
    "        best_model = max(model_results.items(), key=lambda x: x[1]['r2'])\n",
    "        print(\"\\nBest Performing Model:\")\n",
    "        print(f\"Model: {best_model[0]}\")\n",
    "        print(f\"R² Score: {best_model[1]['r2']:.4f}\")\n",
    "        print(f\"MAE: ${best_model[1]['mae']:.2f}\")\n",
    "\n",
    "        return model_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_all_models: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Usage example\n",
    "try:\n",
    "    # Train all models\n",
    "    model_results = train_all_models(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in model training process: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6526a4-24f2-4f44-8a71-cefca1153df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Hyperparameters\n",
    "def tune_linear_regression(X_train_scaled, y_train):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    print(\"\\nLinear Regression (No hyperparameters to tune)\")\n",
    "    return lr\n",
    "\n",
    "def tune_random_forest(X_train_scaled, y_train):\n",
    "    print(\"\\nTuning Random Forest Parameters:\")\n",
    "\n",
    "    # Reduced parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [5],\n",
    "        'min_samples_leaf': [2]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        rf,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best Random Forest Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best R2 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def tune_gradient_boosting(X_train_scaled, y_train):\n",
    "    print(\"\\nTuning Gradient Boosting Parameters:\")\n",
    "\n",
    "    # Reduced parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'min_samples_split': [5],\n",
    "        'subsample': [0.8]\n",
    "    }\n",
    "\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        gb,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best Gradient Boosting Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best R2 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def tune_all_models(X_train_scaled, y_train):\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    try:\n",
    "        # Tune Linear Regression\n",
    "        best_models['Linear Regression'] = tune_linear_regression(X_train_scaled, y_train)\n",
    "\n",
    "        # Tune Random Forest\n",
    "        best_models['Random Forest'] = tune_random_forest(X_train_scaled, y_train)\n",
    "\n",
    "        # Tune Gradient Boosting\n",
    "        best_models['Gradient Boosting'] = tune_gradient_boosting(X_train_scaled, y_train)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTuning interrupted by user. Returning best models found so far\")\n",
    "\n",
    "    return best_models\n",
    "\n",
    "def evaluate_tuned_models(best_models, X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "\n",
    "    print(\"\\nEvaluating Tuned Models:\")\n",
    "\n",
    "    results = {}\n",
    "    for name, model in best_models.items():\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'r2': r2,\n",
    "            'mae': mae\n",
    "        }\n",
    "\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"R2 Score: {r2:.4f}\")\n",
    "        print(f\"MAE: ${mae:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Tune and evaluate models\n",
    "try:\n",
    "    best_models = tune_all_models(X_train_scaled, y_train)\n",
    "    tuned_results = evaluate_tuned_models(best_models, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "    # Find the best performing model\n",
    "    best_model_name = max(tuned_results.items(), key=lambda x: x[1]['r2'])[0]\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"R2 Score: {tuned_results[best_model_name]['r2']:.4f}\")\n",
    "    print(f\"MAE: ${tuned_results[best_model_name]['mae']:.2f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProcess interrupted by user. Showing partial results if available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b46954-1115-408b-b014-5ce78e522dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Feature Importance\n",
    "def analyze_feature_importance(models, preprocessor, original_features):\n",
    "\n",
    "    # Get transformed feature names\n",
    "    def get_feature_names(preprocessor, original_features):\n",
    "        feature_names = []\n",
    "\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'num':\n",
    "                feature_names.extend(features)\n",
    "            elif name == 'cat':\n",
    "                encoded_features = transformer.named_steps['onehot'].get_feature_names_out(features)\n",
    "                feature_names.extend(encoded_features)\n",
    "\n",
    "        return feature_names\n",
    "\n",
    "    # Get transformed feature names\n",
    "    feature_names = get_feature_names(preprocessor, original_features)\n",
    "    importance_dict = {}\n",
    "\n",
    "    try:\n",
    "        for name, model in models.items():\n",
    "            if name == 'Linear Regression':\n",
    "                importance = np.abs(model.coef_)\n",
    "            elif name in ['Random Forest', 'Gradient Boosting']:\n",
    "                importance = model.feature_importances_\n",
    "\n",
    "            # Create DataFrame with feature importance\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            })\n",
    "\n",
    "            # Sort by importance and reset index\n",
    "            importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "            importance_dict[name] = importance_df\n",
    "\n",
    "            # Print feature importance values\n",
    "            print(f\"\\n{name} Feature Importance Values:\")\n",
    "            print(importance_df.to_string(index=True))\n",
    "\n",
    "            # Visualize feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')\n",
    "            plt.title(f'{name} - Feature Importance')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Features')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature importance analysis: {str(e)}\")\n",
    "\n",
    "    return importance_dict\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    # Store original feature names\n",
    "    original_features = ['Company', 'Ram', 'Weight', 'CPU_Brand', 'CPU_Speed',\n",
    "                        'GPU_Brand', 'TouchScreen', 'OpSys', 'SSD', 'HDD']\n",
    "\n",
    "    # Analyze feature importance\n",
    "    importance_dict = analyze_feature_importance(best_models, preprocessor, original_features)\n",
    "\n",
    "    # Compare feature importance across models\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Plot feature importance comparison for top 5 features\n",
    "    model_names = list(importance_dict.keys())\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        top_features = importance_dict[model_name].head(5)\n",
    "\n",
    "        plt.subplot(1, len(model_names), i+1)\n",
    "        sns.barplot(data=top_features, x='Importance', y='Feature')\n",
    "        plt.title(f'{model_name}\\nTop 5 Features')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Features' if i == 0 else '')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in visualization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee73a1-4393-46bc-9df5-d62b999597c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Time Prediction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_laptop_input():\n",
    "    print(\"\\nEnter Laptop Specifications:\")\n",
    "\n",
    "    # Company options\n",
    "    company_options = {\n",
    "        0: 'Acer',\n",
    "        1: 'Apple',\n",
    "        2: 'Asus',\n",
    "        3: 'Dell',\n",
    "        4: 'HP',\n",
    "        5: 'Lenovo',\n",
    "        6: 'MSI',\n",
    "        7: 'Microsoft',\n",
    "        8: 'Toshiba',\n",
    "        9: 'Other'\n",
    "    }\n",
    "    print(\"\\nCompany Options:\")\n",
    "    for key, value in company_options.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    company = int(input(\"Enter company number: \"))\n",
    "    company = company_options[company]\n",
    "\n",
    "    # RAM options\n",
    "    print(\"\\nRAM Options (GB):\")\n",
    "    ram_options = [4, 8, 16, 32, 64]\n",
    "    for i, ram in enumerate(ram_options):\n",
    "        print(f\"{i}: {ram}GB\")\n",
    "    ram_choice = int(input(\"Enter RAM choice: \"))\n",
    "    ram = ram_options[ram_choice]\n",
    "\n",
    "    # Weight\n",
    "    weight = float(input(\"\\nEnter Weight (kg): \"))\n",
    "\n",
    "    # CPU Brand options\n",
    "    cpu_options = {\n",
    "        0: 'Intel',\n",
    "        1: 'AMD',\n",
    "        2: 'Other'\n",
    "    }\n",
    "    print(\"\\nCPU Brand Options:\")\n",
    "    for key, value in cpu_options.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    cpu_brand = int(input(\"Enter CPU brand number: \"))\n",
    "    cpu_brand = cpu_options[cpu_brand]\n",
    "\n",
    "    # CPU Speed\n",
    "    print(\"\\nTypical CPU Speed ranges:\")\n",
    "    print(\"1.8 GHz - 2.2 GHz (Low Power)\")\n",
    "    print(\"2.3 GHz - 2.8 GHz (Mid Range)\")\n",
    "    print(\"2.9 GHz - 3.6 GHz (High Performance)\")\n",
    "    cpu_speed = float(input(\"\\nEnter CPU Speed (GHz): \"))\n",
    "\n",
    "    # GPU Brand options\n",
    "    gpu_options = {\n",
    "        0: 'NVIDIA',\n",
    "        1: 'AMD',\n",
    "        2: 'Intel',\n",
    "        3: 'Other'\n",
    "    }\n",
    "    print(\"\\nGPU Brand Options:\")\n",
    "    for key, value in gpu_options.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    gpu_brand = int(input(\"Enter GPU brand number: \"))\n",
    "    gpu_brand = gpu_options[gpu_brand]\n",
    "\n",
    "    # TouchScreen\n",
    "    touchscreen = int(input(\"\\nTouchscreen (0: No, 1: Yes): \"))\n",
    "\n",
    "    # Operating System options\n",
    "    os_options = {\n",
    "        0: 'Windows',\n",
    "        1: 'Mac',\n",
    "        2: 'Linux',\n",
    "        3: 'Chrome',\n",
    "        4: 'No OS',\n",
    "        5: 'Other'\n",
    "    }\n",
    "    print(\"\\nOperating System Options:\")\n",
    "    for key, value in os_options.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    os = int(input(\"Enter OS number: \"))\n",
    "    os = os_options[os]\n",
    "\n",
    "    # Storage options\n",
    "    print(\"\\nStorage Options:\")\n",
    "    ssd = int(input(\"SSD (0: No, 1: Yes): \"))\n",
    "    hdd = int(input(\"HDD (0: No, 1: Yes): \"))\n",
    "\n",
    "    # Create DataFrame\n",
    "    new_laptop = pd.DataFrame([[\n",
    "        company,\n",
    "        ram,\n",
    "        weight,\n",
    "        cpu_brand,\n",
    "        cpu_speed,\n",
    "        gpu_brand,\n",
    "        touchscreen,\n",
    "        os,\n",
    "        ssd,\n",
    "        hdd\n",
    "    ]], columns=['Company', 'Ram', 'Weight', 'CPU_Brand', 'CPU_Speed',\n",
    "                'GPU_Brand', 'TouchScreen', 'OpSys', 'SSD', 'HDD'])\n",
    "\n",
    "    # Display summary of selections\n",
    "    print(\"\\nSelected Specifications:\")\n",
    "    print(f\"Company: {company}\")\n",
    "    print(f\"RAM: {ram}GB\")\n",
    "    print(f\"Weight: {weight}kg\")\n",
    "    print(f\"CPU Brand: {cpu_brand}\")\n",
    "    print(f\"CPU Speed: {cpu_speed}GHz\")\n",
    "    print(f\"GPU Brand: {gpu_brand}\")\n",
    "    print(f\"TouchScreen: {'Yes' if touchscreen else 'No'}\")\n",
    "    print(f\"Operating System: {os}\")\n",
    "    print(f\"SSD: {'Yes' if ssd else 'No'}\")\n",
    "    print(f\"HDD: {'Yes' if hdd else 'No'}\")\n",
    "\n",
    "    confirm = input(\"\\nConfirm specifications? (yes/no): \")\n",
    "    if confirm.lower() != 'yes':\n",
    "        return create_laptop_input()\n",
    "\n",
    "    return new_laptop\n",
    "\n",
    "def predict_price(models, preprocessor, new_laptop):\n",
    "    predictions = {}\n",
    "\n",
    "    try:\n",
    "        # Transform the input using preprocessor\n",
    "        new_laptop_transformed = preprocessor.transform(new_laptop)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            pred = model.predict(new_laptop_transformed)[0]\n",
    "            predictions[name] = pred\n",
    "\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def display_prediction_results(predictions):\n",
    "    if predictions:\n",
    "        print(\"\\nPredicted Prices:\")\n",
    "\n",
    "        prices = list(predictions.values())\n",
    "        avg_price = np.mean(prices)\n",
    "        min_price = np.min(prices)\n",
    "        max_price = np.max(prices)\n",
    "\n",
    "        for model_name, price in predictions.items():\n",
    "            print(f\"{model_name}: ${price:,.2f}\")\n",
    "\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Average Price: ${avg_price:,.2f}\")\n",
    "        print(f\"Price Range: ${min_price:,.2f} - ${max_price:,.2f}\")\n",
    "    else:\n",
    "        print(\"Unable to make predictions. Please check the input values.\")\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Get laptop specifications from user\n",
    "    new_laptop = create_laptop_input()\n",
    "\n",
    "    # Make predictions using all models\n",
    "    predictions = predict_price(best_models, preprocessor, new_laptop)\n",
    "\n",
    "    # Display results\n",
    "    display_prediction_results(predictions)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"Please try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
